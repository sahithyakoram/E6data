# Agentic Evaluation Framework

A scalable framework for automated evaluation of AI agent responses across multiple dimensions, combining rule-based heuristics and LLM-based scoring for explainable results.

---

## ðŸ”¹ Overview

As AI agents become more powerful, assessing their outputs is critical. This framework scores agent responses across six dimensions:

- Instruction-following
- Coherence & accuracy
- Hallucination detection
- Style matching
- Length penalty
- Assumption control

Supports batch processing, leaderboard generation, and interpretable evaluation reports.

---

## ðŸ”¹ Features

- **Hybrid Scoring**: Combines heuristic rules and LLM-based evaluation.
- **Batch Evaluation**: Process thousands of responses across multiple agents.
- **Explainable Scores**: Each dimension includes an explanation for the score.
- **Leaderboard**: Ranks agents based on weighted scoring.
- **Domain Support**: Works for QA, summarization, and reasoning tasks.

---

## ðŸ”¹ Project Structure

src/evaluation/
â”œâ”€â”€ assumption_control.py        # Flags speculative language
â”œâ”€â”€ coherence_accuracy.py        # Grammar & spelling checks
â”œâ”€â”€ hallucination_detection.py   # Detects unverifiable or exaggerated claims
â”œâ”€â”€ instruction_following.py     # Semantic similarity to prompt
â”œâ”€â”€ style_matching.py            # Penalizes informal/casual language
â”œâ”€â”€ length_penalty.py            # Penalizes overly short or long responses
â”œâ”€â”€ evaluator.py                 # Runs all evaluators on a response
â”œâ”€â”€ evaluate_with_llm.py         # Integrates LLM-based AI judge
â”œâ”€â”€ batch_runner.py              # Processes batches, outputs leaderboard
â”œâ”€â”€ data_loader.py               # Converts dataset to internal format
â”œâ”€â”€ generate_batch.py            # Generates synthetic agent prompts/responses
â””â”€â”€ tests/                       # Unit tests for evaluators


---

## ðŸ”¹ Installation

1. Clone the repository:

```bash
git clone https://github.com/<your-username>/agentic-evaluation-framework.git
cd agentic-evaluation-framework

python -m venv .venv
source .venv/bin/activate   # Linux/Mac
# .venv\Scripts\activate    # Windows

pip install -r requirements.txt

export GROQ_API_KEY="your_groq_api_key"   # Linux/Mac
# set GROQ_API_KEY=your_groq_api_key      # Windows

